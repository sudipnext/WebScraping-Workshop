{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f63a070",
   "metadata": {},
   "source": [
    "## <span style=\"color:#3366ff; font-weight:bold;\">Web Scraping Workshop - Dec 23, Saturday 2023</span>\n",
    "### Instructor: <span style=\"font-weight:normal;\">Sudip Parajuli</span>\n",
    "##### Organized by: EXCESS as X-Tech Studio 4.0 PreEvent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec23487",
   "metadata": {},
   "source": [
    "## **Module 1: Introduction to Web Scraping**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707e9a0",
   "metadata": {},
   "source": [
    "### What is Web Scraping?\n",
    "- üï∏Ô∏è Web Scraping: the magical art of extracting data from any website!\n",
    "\n",
    "### Why is Web Scraping Valuable?\n",
    "- üåü Because it turns random web pages into treasure troves of information!\n",
    "\n",
    "### Real World Applications of Web Scraping\n",
    "- üèõÔ∏è Academic research: Gathering data for scholarly studies\n",
    "- üìà Business insights: Accessing market trends at your fingertips\n",
    "- üõí Price monitoring: Finding the best deals for savvy shoppers\n",
    "- üîç Competitive analysis: Keeping an eye on the competition\n",
    "- üóûÔ∏è News aggregation: Bringing the headlines to you\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa058e",
   "metadata": {},
   "source": [
    "## **Module 2: HTML Basics and Inspecting Web Pages**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>HTML Basics and Inspecting Web Pages</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Welcome to HTML Basics!</h1>\n",
    "    <p>HTML (HyperText Markup Language) is the standard language for creating web pages. It provides the structure for content on the web.</p>\n",
    "    \n",
    "    <h2>Basic HTML Structure</h2>\n",
    "    <p>HTML documents consist of elements. Each element begins with an opening tag and ends with a closing tag.</p>\n",
    "    <p>&lt;element&gt;Content&lt;/element&gt;</p>\n",
    "    \n",
    "    <h2>Inspecting Web Pages</h2>\n",
    "    <p>Inspecting web pages allows us to view and understand the structure, styles, and content of a webpage using browser developer tools.</p>\n",
    "    <p>To inspect a web page:</p>\n",
    "    <ol>\n",
    "        <li>Right-click on an element.</li>\n",
    "        <li>Select \"Inspect\" or \"Inspect Element\".</li>\n",
    "        <li>The developer tools panel will open, displaying the HTML and CSS of the element.</li>\n",
    "    </ol>\n",
    "    \n",
    "    <h2>Example Element</h2>\n",
    "    <p>This is an example paragraph. You can inspect this paragraph to see its HTML structure.</p>\n",
    "    \n",
    "    <h2>Example Table</h2>\n",
    "    <table border=\"1\">\n",
    "        <caption>Sample Table</caption>\n",
    "        <tr>\n",
    "            <th>Header 1</th>\n",
    "            <th>Header 2</th>\n",
    "            <th>Header 3</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Row 1, Cell 1</td>\n",
    "            <td>Row 1, Cell 2</td>\n",
    "            <td>Row 1, Cell 3</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Row 2, Cell 1</td>\n",
    "            <td>Row 2, Cell 2</td>\n",
    "            <td>Row 2, Cell 3</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    \n",
    "    <footer>\n",
    "        <p>You can put your footer contents here!</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe0204",
   "metadata": {},
   "source": [
    "### **<span style=\"color: lightgray;\">Output:- This is how the rendered page should look like</span>**\n",
    "\n",
    "![Rendered Image](page.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4e80b",
   "metadata": {},
   "source": [
    "### **Intro to HTML Tags and Attributes**\n",
    "\n",
    "HTML (Hypertext Markup Language) utilizes tags and attributes to structure and define content within a web page. Tags are used to mark the beginning and end of elements, while attributes provide additional information about the elements.\n",
    "\n",
    "**HTML Tags**\n",
    "\n",
    "Tags are enclosed in angle brackets <>, and most come in pairs‚Äîan opening tag and a closing tag.\n",
    "\n",
    "\n",
    "**Example:**\n",
    "\n",
    "\n",
    "Putting Text\n",
    "```html\n",
    "<p>This is a paragraph tag. It has an opening <p> and a closing </p> tag.</p>\n",
    "<h1> to <h6> Tags (Headings)\n",
    "Defines headings with varying sizes, where <h1> is the largest and <h6> is the smallest.   \n",
    "```\n",
    "Anchor Tag\n",
    "- Creates hyperlinks to other web pages or resources.\n",
    "```html\n",
    "<a href=\"https://example.com\">Visit our website</a>\n",
    "```\n",
    "\n",
    "Image\n",
    "- Embeds images into a web page.\n",
    "```html\n",
    "<img src=\"image.jpg\" alt=\"Description of the image\">\n",
    "```\n",
    "\n",
    "Unordered List Tag\n",
    "- Creates an unordered (bulleted) list.\n",
    "For ordered just replace the ul with ol\n",
    "```html\n",
    "<ul>\n",
    "    <li>Item 1</li>\n",
    "    <li>Item 2</li>\n",
    "</ul>\n",
    "```\n",
    "\n",
    "Table Tag\n",
    "- Creates a table structure with rows and columns.\n",
    "\n",
    "```html\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Header 1</th>\n",
    "        <th>Header 2</th>\n",
    "        <th>Header 3</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Row 1, Cell 1</td>\n",
    "        <td>Row 1, Cell 2</td>\n",
    "        <td>Row 1, Cell 3</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Row 2, Cell 1</td>\n",
    "        <td>Row 2, Cell 2</td>\n",
    "        <td>Row 2, Cell 3</td>\n",
    "    </tr>\n",
    "</table>\n",
    "```\n",
    "Form Tag\n",
    "- Defines a form for user input with input fields and a submit button.\n",
    "```html\n",
    "<form action=\"/submit-form\" method=\"post\">\n",
    "    <label for=\"username\">Username:</label>\n",
    "    <input type=\"text\" id=\"username\" name=\"username\"><br><br>\n",
    "    <label for=\"password\">Password:</label>\n",
    "    <input type=\"password\" id=\"password\" name=\"password\"><br><br>\n",
    "    <input type=\"submit\" value=\"Submit\">\n",
    "</form>\n",
    "```\n",
    "\n",
    "Div Tag\n",
    "- Creates a division or a container that can be styled using CSS.\n",
    "```html\n",
    "<div>\n",
    "    <!-- Content to be enclosed within the div -->\n",
    "</div>\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3b2d8",
   "metadata": {},
   "source": [
    "### **Overview of CSS selectors for targeting elements**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106752e9",
   "metadata": {},
   "source": [
    "### **Inspecting Web Pages**\n",
    "\n",
    "Inspecting web pages allows us to view and understand the structure, styles, and content of a webpage using browser developer tools.\n",
    "\n",
    "**To inspect a web page:**\n",
    "1. Right-click on an element.\n",
    "2. Select \"Inspect\" or \"Inspect Element\".\n",
    "3. The developer tools panel will open, displaying the HTML and CSS of the element.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1ba31",
   "metadata": {},
   "source": [
    "## **Module 3: Extracting Data with CSS Selectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f73ef7",
   "metadata": {},
   "source": [
    "### **Overview of CSS selectors for targeting elements**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c7c9f",
   "metadata": {},
   "source": [
    "CSS selectors are vital tools in web scraping, allowing precise targeting and extraction of data from HTML documents. Here's a concise breakdown:\n",
    "\n",
    "1. **Understanding Selectors**: CSS selectors identify elements based on attributes, structure, and relationships. For instance:\n",
    "   - `p` targets all paragraphs.\n",
    "   - `.class` selects elements with a specific class.\n",
    "   - `#id` targets elements by their unique ID.\n",
    "   - `a[href=\"https://example.com\"]` selects links with the specified URL.\n",
    "\n",
    "2. **Types of Selectors**: They include:\n",
    "   - **Element**: Selects based on tag names.\n",
    "   - **Class**: Identifies elements by class names.\n",
    "   - **ID**: Targets elements with unique identifiers.\n",
    "   - **Attribute**: Selects elements by specific attributes.\n",
    "   - **Combination**: Uses multiple selectors for precision.\n",
    "\n",
    "3. **Application in Web Scraping**: CSS selectors aid in pinpointing and extracting desired data efficiently. For example:\n",
    "   - `div#content` targets a `<div>` element with the ID of 'content'.\n",
    "   - `.price` selects all elements with the class 'price'.\n",
    "   - `table td` targets all table cells.\n",
    "   - `ul > li` selects direct list items within unordered lists.\n",
    "\n",
    "4. **Challenges**: Complex structures or dynamic elements require adaptable selectors. Examples include:\n",
    "   - Selecting elements nested within others.\n",
    "   - Handling dynamically generated content or changing classes.\n",
    "\n",
    "CSS selectors play a pivotal role in effective data extraction during web scraping, offering accuracy and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595bc54",
   "metadata": {},
   "source": [
    "### **Locating and targeting elements using CSS Selectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f01070",
   "metadata": {},
   "source": [
    "In web scraping, efficiently locating and targeting specific elements within the HTML structure is crucial for accurate data extraction. CSS Selectors are instrumental in this process, offering various techniques:\n",
    "\n",
    "#### 1. **Locating Elements**\n",
    "\n",
    "CSS Selectors allow precise identification of elements based on attributes, structure, and relationships in the HTML document:\n",
    "\n",
    "- **Tag Names**: Target elements by their tag names like `p`, `div`, `a`.\n",
    "- **Class and ID**: Use `.class` and `#id` selectors to locate elements by their class and unique ID.\n",
    "- **Attribute Selectors**: Locate elements based on attributes, e.g., `[data-type=\"header\"]`.\n",
    "\n",
    "#### 2. **Targeting Elements**\n",
    "\n",
    "After locating elements, further target specific content or data within them using CSS Selectors:\n",
    "\n",
    "- **Text Content**: Access and extract text content using selectors like `p`, `h1`.\n",
    "- **Links**: Target hyperlinks with `a` or specific attributes like `a[href^=\"https://\"]`.\n",
    "- **Images**: Access images using `img` or selectors based on image attributes.\n",
    "\n",
    "#### 3. **Specificity and Precision**\n",
    "\n",
    "CSS Selectors offer varying levels of specificity for precise targeting:\n",
    "\n",
    "- **Combination Selectors**: Combine classes, tags, and attributes for targeted extraction, e.g., `.class > p[data-type=\"info\"]`.\n",
    "- **Nested Elements**: Select elements nested within others, accessing specific data layers deep within the structure.\n",
    "\n",
    "#### 4. **Extensive Examples**\n",
    "\n",
    "```css\n",
    "/* Examples showcasing CSS Selectors for data extraction */\n",
    "\n",
    "/* Selecting specific class */\n",
    ".info {\n",
    "    /* Styles or extraction */\n",
    "}\n",
    "\n",
    "/* Targeting elements by ID */\n",
    "#main-content {\n",
    "    /* Styles or extraction */\n",
    "}\n",
    "\n",
    "/* Accessing nested elements */\n",
    ".container > div:nth-child(2) > ul > li a {\n",
    "    /* Styles or extraction */\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55910a30",
   "metadata": {},
   "source": [
    "### **Extracting data from HTML elements (text, attributes, etc.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727fafee",
   "metadata": {},
   "source": [
    "Web scraping involves precise extraction of data from HTML elements. Here's a detailed breakdown:\n",
    "\n",
    "#### 1. **Text Content Extraction**\n",
    "\n",
    "- **Using Element Tags**: Extract text within specific elements using their tags:\n",
    "  \n",
    "  ```html\n",
    "  <p>This is a paragraph text.</p>\n",
    "  <h1>Heading text</h1>\n",
    "```\n",
    "- **Class or ID Targeting**: Access text within specific classes or IDs:\n",
    "\n",
    "    ```html\n",
    "    <p class=\"description\">Product description text.</p>\n",
    "    <div id=\"main-content\">Main content text.</div>\n",
    "\n",
    "    ```\n",
    "    \n",
    "#### 2. **Attributes Extraction**\n",
    "\n",
    "- **Hyperlinks (href)**: Extract URLs from anchor tags:\n",
    "    ```html\n",
    "<a href=\"https://example.com\">Visit our website</a>\n",
    "\n",
    "    ```\n",
    "- **Image URLs (src)**: Access image URLs from image tags:\n",
    "    ```html\n",
    "<img src=\"image.jpg\" alt=\"Image description\">\n",
    "\n",
    "    ```\n",
    "    \n",
    "#### 3. **Attribute Values**\n",
    "\n",
    "- **Extracting Specific Attributes**: Retrieve values of specific attributes:\n",
    "    ```html\n",
    "<div data-category=\"tech\">Technology category</div>\n",
    "<img src=\"logo.png\" alt=\"Company Logo\" title=\"Company XYZ\">\n",
    "\n",
    "    ```\n",
    "\n",
    "#### 4. **Combining Selectors for Targeted Extraction**\n",
    "\n",
    "- **Nested Elements**: Combine selectors for extraction within nested structures:\n",
    "    ```html\n",
    "<div class=\"parent\">\n",
    "    <div class=\"child\">Nested content</div>\n",
    "</div>\n",
    "\n",
    "    ```\n",
    "    \n",
    "#### 5. **Advanced Techniques**\n",
    "\n",
    "- **Regular Expressions and Conditional Extraction:**\n",
    "    ```html\n",
    "<div class=\"featured\">\n",
    "    <h2>Featured Product</h2>\n",
    "    <p class=\"discount\">20% off!</p>\n",
    "</div>\n",
    "\n",
    "    ```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471bf5e",
   "metadata": {},
   "source": [
    "## **Module 4: Handling Dynamic Web Content**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef49306a",
   "metadata": {},
   "source": [
    "### **Understanding dynamic web content**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5524ab3",
   "metadata": {},
   "source": [
    "Dynamic web content refers to elements on a webpage that change or update without requiring a full page reload. Understanding these dynamics is crucial in web scraping due to the complexity introduced by their ever-changing nature. Here's an in-depth exploration:\n",
    "\n",
    "#### 1. **Characteristics of Dynamic Web Pages**\n",
    "\n",
    "- **JavaScript Interactions**: Elements or content updated via JavaScript interactions or events.\n",
    "- **Asynchronous Loading**: Content loaded independently, often asynchronously without a page refresh.\n",
    "- **DOM Manipulation**: Alteration of the Document Object Model (DOM) structure based on user actions or server responses.\n",
    "\n",
    "#### 2. **Behavioral Aspects**\n",
    "\n",
    "- **Real-time Updates**: Data or content that refreshes automatically or periodically.\n",
    "- **Interactive Elements**: Features that respond to user inputs or triggers.\n",
    "\n",
    "#### 3. **Rendering Mechanisms**\n",
    "\n",
    "- **Client-Side Rendering**: Content rendered on the client-side using JavaScript frameworks like React, Angular, or Vue.js.\n",
    "- **Server-Side Rendering**: Pages generated dynamically on the server and sent to the client.\n",
    "\n",
    "#### 4. **Impact on Web Scraping**\n",
    "\n",
    "- **Complexity in Data Retrieval**: Challenges in accessing dynamically loaded content through traditional scraping methods.\n",
    "- **Need for Adaptability**: Requires adaptable scraping techniques to capture data rendered after page load.\n",
    "- **Handling Asynchronous Requests**: Dealing with content loaded via AJAX or other asynchronous mechanisms.\n",
    "\n",
    "#### 5. **Tools and Strategies for Understanding Dynamics**\n",
    "\n",
    "- **Developer Tools**: Leveraging browser developer tools to inspect dynamic elements and interactions.\n",
    "- **Network Analysis**: Understanding network requests to identify dynamically loaded data.\n",
    "- **Testing Environments**: Using testing frameworks to simulate dynamic content for scraping exploration.\n",
    "\n",
    "Understanding the behavior and mechanisms of dynamic web content is fundamental in effectively extracting data during web scraping. Mastery of these dynamics empowers scraping strategies to adapt and capture real-time or constantly changing information from modern web applications.\n",
    "\n",
    "example:- https://today.yougov.com/ratings/entertainment/fame/people/all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac662f4f",
   "metadata": {},
   "source": [
    "### **Dealing with JavaScript-driven Websites**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874bc1c",
   "metadata": {},
   "source": [
    "JavaScript-driven websites present unique challenges in web scraping due to their reliance on client-side scripting for content rendering and interactions. Effectively scraping data from such sites involves understanding and addressing these complexities:\n",
    "\n",
    "#### 1. **JavaScript Rendered Content**\n",
    "\n",
    "- **Dynamic Element Creation**: Elements generated dynamically via JavaScript after page load.\n",
    "- **Delayed Data Loading**: Content loaded asynchronously after initial page load events.\n",
    "\n",
    "#### 2. **Handling Asynchronous Requests**\n",
    "\n",
    "- **AJAX Calls**: Data loaded through asynchronous requests, requiring specialized handling.\n",
    "- **Wait Time Consideration**: Dealing with delays as content loads progressively.\n",
    "\n",
    "#### 3. **Approaches for Scraping JavaScript-Driven Sites**\n",
    "\n",
    "- **Headless Browsers**: Leveraging tools like Puppeteer or Selenium to mimic browser behavior and access dynamically rendered content.\n",
    "- **JavaScript Execution**: Executing JavaScript code within scraping scripts to access dynamically generated elements.\n",
    "- **API Interactions**: Utilizing exposed APIs or endpoints to retrieve data directly.\n",
    "\n",
    "#### 4. **Challenges and Solutions**\n",
    "\n",
    "- **Dynamic Element Identification**: Difficulty in targeting elements loaded dynamically, requiring adaptive selectors or techniques.\n",
    "- **Handling JavaScript Events**: Capturing data triggered by user interactions or JavaScript events.\n",
    "\n",
    "#### 5. **Adaptation and Tool Selection**\n",
    "\n",
    "- **Adapting Scraping Strategies**: Modifying scraping strategies to handle dynamic content updates or DOM manipulations.\n",
    "- **Tool Selection**: Choosing appropriate scraping libraries or frameworks that support JavaScript rendering and dynamic content extraction.\n",
    "\n",
    "Dealing with JavaScript-driven websites demands specialized approaches and tools to effectively scrape data. Mastering these techniques enables access to the rich and interactive content offered by modern web applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890ebde",
   "metadata": {},
   "source": [
    "### **Introduction to headless browsers and their role in web scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1b3b0",
   "metadata": {},
   "source": [
    "Headless browsers simulate the behavior of a regular web browser without a graphical user interface (GUI). They play a vital role in web scraping by providing a programmatic way to interact with and extract data from web pages. Here's an overview:\n",
    "\n",
    "#### 1. **What are Headless Browsers?**\n",
    "\n",
    "- **GUI-Less Browsing**: Operate as browsers without a visual interface, running in the background.\n",
    "- **Automated Web Interaction**: Enable automated browsing, navigation, and content retrieval.\n",
    "\n",
    "#### 2. **Role in Web Scraping**\n",
    "\n",
    "- **Automated Interaction**: Allows scripting interactions with web pages, enabling data extraction without human intervention.\n",
    "- **Rendering and JavaScript Support**: Offers rendering capabilities and supports JavaScript execution for accessing dynamically rendered content.\n",
    "\n",
    "#### 3. **Popular Headless Browsers**\n",
    "\n",
    "- **Puppeteer**: Developed by Google, provides a high-level API to control headless Chrome or Chromium browsers.\n",
    "- **Selenium WebDriver**: Supports headless mode across various browsers, allowing automation and scraping tasks.\n",
    "\n",
    "#### 4. **Advantages in Web Scraping**\n",
    "\n",
    "- **JavaScript Execution**: Executes JavaScript and renders dynamic content, crucial for scraping modern web applications.\n",
    "- **Automation Capabilities**: Enables scripted interactions like form submissions, clicking elements, and navigating through pages.\n",
    "- **Efficiency and Scalability**: Automates scraping tasks, enhancing efficiency and scalability in data extraction processes.\n",
    "\n",
    "#### 5. **Use Cases in Web Scraping**\n",
    "\n",
    "- **Data Collection**: Extracting structured data from websites with dynamic content or complex interactions.\n",
    "- **Testing and Automation**: Automating website testing, user interaction simulations, and regression testing.\n",
    "\n",
    "#### 6. **Role in Handling Dynamic Content**\n",
    "\n",
    "- **Addressing JavaScript Rendered Content**: Essential for scraping data from websites relying heavily on client-side scripting.\n",
    "\n",
    "Headless browsers serve as powerful tools in web scraping, offering the capability to automate interactions, access dynamic content, and retrieve data from modern and dynamic web pages without human intervention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345e2ea",
   "metadata": {},
   "source": [
    "## **Module 5: Web Scraping Tools and Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfccd06",
   "metadata": {},
   "source": [
    "### **Introduction to popular web scraping tools and libraries (e.g: BeautifulSoup, Scrapy, Selenium, MechanicalSoup)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68fe948",
   "metadata": {},
   "source": [
    "Web scraping tools and libraries simplify the process of extracting data from websites, offering various functionalities and approaches to facilitate scraping tasks. Here's an overview of some widely-used tools:\n",
    "\n",
    "#### 1. **Beautiful Soup**\n",
    "\n",
    "- **Parsing HTML and XML**: Provides a simple API for navigating and searching parsed data from HTML or XML documents.\n",
    "- **Ease of Use**: Allows quick extraction of data by handling complexities in HTML structure.\n",
    "- **Integration with Parsing Libraries**: Often used alongside libraries like `requests` for web page retrieval.\n",
    "\n",
    "#### 2. **Scrapy**\n",
    "\n",
    "- **Full-Fledged Framework**: Offers a complete framework for scraping, handling requests, and managing spiders.\n",
    "- **Asynchronous Requests**: Supports concurrent requests, enhancing scraping speed and efficiency.\n",
    "- **Modularity and Extensibility**: Highly customizable and allows integration with middleware and extensions.\n",
    "\n",
    "#### 3. **Selenium**\n",
    "\n",
    "- **Browser Automation**: Provides automated testing and web scraping by simulating browser interactions.\n",
    "- **Dynamic Content Handling**: Capable of handling JavaScript-rendered content and interactions.\n",
    "- **Cross-Browser Support**: Works with multiple browsers and supports headless modes for automated scraping.\n",
    "\n",
    "#### 4. **MechanicalSoup**\n",
    "\n",
    "- **Simplified Interface**: Wrapper around `requests` and `Beautiful Soup`, simplifying interaction with web pages.\n",
    "- **Form Submission and Navigation**: Streamlines form submission and navigation tasks during scraping.\n",
    "- **Ease of Use for Simple Tasks**: Ideal for simpler scraping tasks or when combining `requests` and `Beautiful Soup`.\n",
    "\n",
    "#### 5. **Use Cases and Selection Considerations**\n",
    "\n",
    "- **Project Complexity**: Selection based on the complexity of scraping tasks and the structure of target websites.\n",
    "- **Handling Dynamic Content**: Consideration for tools capable of handling JavaScript-rendered content.\n",
    "- **Community Support and Documentation**: Importance of active communities and comprehensive documentation for support.\n",
    "\n",
    "#### 6. **Combining Tools for Enhanced Functionality**\n",
    "\n",
    "- **Integration Potential**: Utilizing combinations of tools for enhanced scraping capabilities, e.g., using Selenium for dynamic content handling alongside Beautiful Soup for parsing.\n",
    "\n",
    "These tools and libraries serve as valuable resources for web scraping, each offering distinct features, functionalities, and approaches catering to different scraping requirements and complexities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90553104",
   "metadata": {},
   "source": [
    "### **Pros and Cons of different tools**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ae928",
   "metadata": {},
   "source": [
    "When selecting a web scraping tool, it's essential to consider the strengths and limitations each tool offers:\n",
    "\n",
    "#### **Beautiful Soup**\n",
    "\n",
    "**Pros:**\n",
    "- Easy to Learn: Simple API for parsing HTML and XML, suitable for beginners.\n",
    "- Pythonic Approach: Integrates seamlessly with Python for data extraction tasks.\n",
    "- Parsing Complexities: Handles intricate HTML structures and navigates through nested elements.\n",
    "\n",
    "**Cons:**\n",
    "- Limited Fetching Abilities: Requires additional libraries like `requests` for fetching web pages.\n",
    "- Lacks Automation: Not suitable for handling complex automation or dynamic content.\n",
    "\n",
    "#### **Scrapy**\n",
    "\n",
    "**Pros:**\n",
    "- Full-Fledged Framework: Offers a complete scraping framework with features for managing spiders, pipelines, and middleware.\n",
    "- Asynchronous Processing: Supports concurrent requests, enhancing scraping speed.\n",
    "- Scalability: Suitable for large-scale scraping projects due to its modularity and extensibility.\n",
    "\n",
    "**Cons:**\n",
    "- Learning Curve: Steeper learning curve compared to simpler libraries due to its comprehensive nature.\n",
    "- Complexity Overhead: Might be overkill for smaller or less complex scraping tasks.\n",
    "\n",
    "#### **Selenium**\n",
    "\n",
    "**Pros:**\n",
    "- Browser Automation: Simulates browser interactions, suitable for handling JavaScript-rendered content.\n",
    "- Dynamic Content Handling: Capable of handling dynamic content and interactions.\n",
    "- Cross-Browser Compatibility: Works across multiple browsers for testing and scraping.\n",
    "\n",
    "**Cons:**\n",
    "- Slower Execution: Running a full browser can be slower compared to parsing HTML directly.\n",
    "- Setup Overhead: Requires browser drivers and setup configurations for different browsers.\n",
    "\n",
    "#### **MechanicalSoup**\n",
    "\n",
    "**Pros:**\n",
    "- Simplified Interface: Combines `requests` and `Beautiful Soup`, streamlining basic scraping tasks.\n",
    "- Ease of Use: User-friendly for simpler scraping tasks or when combining `requests` and `Beautiful Soup`.\n",
    "\n",
    "**Cons:**\n",
    "- Limited Functionality: Not as feature-rich or capable as more comprehensive frameworks like Scrapy.\n",
    "- Handling Dynamic Content: Limited capability in handling JavaScript-rendered content or complex interactions.\n",
    "\n",
    "Selecting the right tool depends on the specific requirements of the scraping task, considering factors like project complexity, handling of dynamic content, scalability, and ease of use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229ce2f",
   "metadata": {},
   "source": [
    "### **Selecting the right tool for your scraping needs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0eca0e",
   "metadata": {},
   "source": [
    "Choosing the most suitable web scraping tool requires an understanding of project requirements and the capabilities of available tools. Consider the following factors:\n",
    "\n",
    "#### 1. **Project Complexity**\n",
    "\n",
    "- **Simple Tasks**: For basic data extraction from static web pages, lightweight libraries like Beautiful Soup or MechanicalSoup suffice.\n",
    "- **Complex Tasks**: Projects involving large-scale scraping, handling dynamic content, or complex structures may benefit from comprehensive frameworks like Scrapy or Selenium.\n",
    "\n",
    "#### 2. **Handling Dynamic Content**\n",
    "\n",
    "- **JavaScript-Heavy Sites**: Websites with heavy reliance on JavaScript-rendered content necessitate tools capable of handling dynamic elements, like Selenium or headless browsers.\n",
    "\n",
    "#### 3. **Scalability**\n",
    "\n",
    "- **Large-Scale Scraping**: Projects requiring scalability, concurrent requests, and modular design favor frameworks like Scrapy due to their scalability and extensibility.\n",
    "\n",
    "#### 4. **Learning Curve**\n",
    "\n",
    "- **Ease of Use**: Consider the learning curve associated with each tool. Simple tasks might benefit from straightforward libraries like Beautiful Soup or MechanicalSoup, while complex projects might justify the learning curve of Scrapy or Selenium.\n",
    "\n",
    "#### 5. **Performance and Speed**\n",
    "\n",
    "- **Execution Speed**: Evaluate the performance impact of each tool. Headless browsers like Selenium might be slower due to browser simulation, while direct parsers like Beautiful Soup offer faster parsing.\n",
    "\n",
    "#### 6. **Community Support and Documentation**\n",
    "\n",
    "- **Active Community**: Tools with an active community and comprehensive documentation provide valuable resources for troubleshooting and support.\n",
    "\n",
    "#### 7. **Customization and Flexibility**\n",
    "\n",
    "- **Modularity and Customization**: Frameworks like Scrapy offer high customization and modularity for tailored scraping needs, while simpler libraries might lack this flexibility.\n",
    "\n",
    "#### 8. **Testing and Experimentation**\n",
    "\n",
    "- **Trial and Error**: Consider experimenting with different tools to find the best fit for your specific project needs. Conduct small-scale tests to assess compatibility and efficiency.\n",
    "\n",
    "Selecting the right tool involves aligning project requirements with the strengths and limitations of available scraping tools, aiming for an optimal balance between functionality, ease of use, performance, and scalability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acefef1",
   "metadata": {},
   "source": [
    "## **Module 6: Best Practices and Ethical Considerations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c197c5",
   "metadata": {},
   "source": [
    "Module 6 focuses on ethical conduct and best practices when engaging in web scraping activities. Understanding and adhering to ethical guidelines are essential in maintaining the integrity of data extraction and respecting the rights and policies of websites.\n",
    "\n",
    "### **Key Topics Covered:**\n",
    "\n",
    "#### 1. **Respect for Website Guidelines**\n",
    "\n",
    "- **Robots.txt Compliance**: Understanding and adhering to the guidelines outlined in the robots.txt file, respecting site-specific rules and directives.\n",
    "- **Terms of Service and Legal Boundaries**: Awareness of website terms of service and legal boundaries governing data collection activities.\n",
    "\n",
    "#### 2. **Data Privacy and Security**\n",
    "\n",
    "- **Sensitive Data Handling**: Exercising caution when scraping and handling personally identifiable or sensitive information.\n",
    "- **Storage and Security Practices**: Implementing secure storage methods and encryption for scraped data to maintain confidentiality.\n",
    "\n",
    "#### 3. **Rate Limiting and Politeness**\n",
    "\n",
    "- **Responsible Scraping Rates**: Implementing rate limiting to prevent overwhelming servers and showing politeness towards server resources.\n",
    "- **Monitoring and Adjustment**: Strategies for monitoring server responses and adjusting scraping rates accordingly.\n",
    "\n",
    "#### 4. **Code Quality and Maintenance**\n",
    "\n",
    "- **Robust and Maintainable Code**: Writing clean, well-documented, and maintainable code for efficient and scalable scraping.\n",
    "- **Error Handling and Logging**: Implementing robust error handling and logging mechanisms to ensure data integrity and traceability.\n",
    "\n",
    "### **Importance of Best Practices**\n",
    "\n",
    "- **Legal Compliance**: Adherence to legal frameworks, copyright laws, and data protection regulations to prevent legal complications.\n",
    "- **Maintaining Reputability**: Upholding ethical scraping practices fosters trust and credibility within the data science community and prevents backlash from website owners.\n",
    "\n",
    "### **Ethical Considerations**\n",
    "\n",
    "- **Transparency**: Being transparent about scraping activities and purposes, avoiding misleading or harmful practices.\n",
    "- **Respect for Website Resources**: Ensuring scraping activities do not unduly burden website servers or cause disruptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc75f8",
   "metadata": {},
   "source": [
    "## **Module 7: Let's Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6481e",
   "metadata": {},
   "source": [
    "### Installing Necessary Libraries for this workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a928a4",
   "metadata": {},
   "source": [
    "Note: The exclamation mark (!) preceding a command within a Jupyter Notebook or a similar environment indicates that the command should be executed in the terminal or command prompt rather than within the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000ec1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7529f",
   "metadata": {},
   "source": [
    "### Starting With Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37177790",
   "metadata": {},
   "source": [
    "Requests Short Overview\n",
    "[More in Requests documentation](https://requests.readthedocs.io/en/latest/)\n",
    "\n",
    "```python\n",
    "import requests\n",
    "#url\n",
    "res = requests.get('https://example.com')\n",
    "#payload\n",
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "res = requests.get('https://example.com/get', params=payload)\n",
    "#headers\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "res = requests.get('https://example.com', headers=headers)\n",
    "#timeout\n",
    "res = requests.get('https://example.com', timeout=5)\n",
    "#auth\n",
    "res = requests.get('https://example.com', auth=('username', 'password'))\n",
    "#proxies\n",
    "proxies = {'http': 'http://proxy.example.com:8080', 'https': 'https://proxy.example.com:8080'}\n",
    "res = requests.get('https://example.com', proxies=proxies)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2970fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(\"https://www.celebheights.com/s/allA.html\")\n",
    "if res.ok:\n",
    "    #your code here\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f0d3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup= BeautifulSoup(res.content, parser=\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c0311",
   "metadata": {},
   "source": [
    "### Getting all the names of the Celebrity and their heights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251d567",
   "metadata": {},
   "source": [
    "Possible soup methods\n",
    "\n",
    "```python\n",
    "soup.find_all('a')\n",
    "soup.find('div', class_='captionBottom')\n",
    "\n",
    "class_select = soup.select(\".class_name\")\n",
    "or\n",
    "soup.find_all('div', class_='class_name')\n",
    "\n",
    "id_select = soup.select(\"#id_name\")\n",
    "# here we are getting the first div of the html content and getting the text within it\n",
    "soup.find('div').get_text()\n",
    "# we can use | to separate the two texts within a html element\n",
    "# <div>\n",
    "# <p>Hello<span>World</span><p>\n",
    "# </div>\n",
    "\n",
    "soup.find('div').get_text(\"|\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb97c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here \n",
    "names_div = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73907540",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_strings = []\n",
    "heights_of_celebs = []\n",
    "for name in names_div:\n",
    "    pass\n",
    "    #your code here, remember to remove the pass\n",
    "    #hint: you can use the above technique to filter out and get the data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121f81c",
   "metadata": {},
   "source": [
    "### Exporting Data to CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a1227",
   "metadata": {},
   "source": [
    "Two ways of writing a data from an array to a csv file\n",
    "- 1 \n",
    "\n",
    "```python\n",
    "import csv\n",
    "\n",
    "names_strings = ['Alice', 'Bob', 'Charlie']\n",
    "heights_of_celebs = [165, 180, 175]\n",
    "\n",
    "file = open('heights.csv', mode='w', newline='', encoding='utf-8')\n",
    "writer = csv.writer(file)\n",
    "writer.writerow(['Name', 'Height'])\n",
    "for name, height in zip(names_strings, heights_of_celebs):\n",
    "    writer.writerow([name, height])\n",
    "\n",
    "file.close()  # Remember to close the file after writing\n",
    "\n",
    "```\n",
    "\n",
    "- 2\n",
    "\n",
    "```python\n",
    "import csv\n",
    "\n",
    "names_strings = ['Alice', 'Bob', 'Charlie', 'D≈çmo']\n",
    "heights_of_celebs = [165, 180, 175, 170]\n",
    "\n",
    "with open('heights.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Name', 'Height'])\n",
    "    for name, height in zip(names_strings, heights_of_celebs):\n",
    "        writer.writerow([name, height])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22d79270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the code similar to above to save that to a csv file\n",
    "#your code here ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0726df",
   "metadata": {},
   "source": [
    "### **Using Selenium**\n",
    "\n",
    "##### Note:- \n",
    "It requires the chrome binary or the chrome installed to your machine inorder for this to work.\n",
    "Your Task:-\n",
    "Try Running up the selenium on your machine\n",
    "https://tecadmin.net/setup-selenium-with-python-on-ubuntu-debian/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "544e8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try Running This on your home after setting up the selenium\\\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set Chrome options to run in headless mode\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# Create a WebDriver instance using Chrome\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Perform tasks with the headless WebDriver\n",
    "driver.get(\"https://www.google.com\")\n",
    "# Other interactions or operations can be performed here\n",
    "\n",
    "# Don't forget to quit the driver once the tasks are done\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from requests.exceptions import Timeout\n",
    "\n",
    "# Create an empty list to store the responses\n",
    "responses = []\n",
    "\n",
    "def fetch_data(name):\n",
    "    try:\n",
    "        api_url = f'https://www.famousbirthdays.com/api/autocomplete?term={name}'\n",
    "        response = requests.get(api_url, timeout=10)\n",
    "        \n",
    "        if response.status_code == requests.codes.ok:\n",
    "            response_data = response.json()\n",
    "            if response_data:\n",
    "                 newDict = {}\n",
    "                 newDict[\"data\"] = response_data[\"suggestions\"][0]['data']\n",
    "                 newDict[\"value\"] = (response_data[\"suggestions\"][0]['value'])\n",
    "                 responses.append(newDict)\n",
    "                 print(newDict)\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code, response.text)\n",
    "    \n",
    "    except Timeout:\n",
    "        print(\"Connection timeout occurred for\", name)\n",
    "\n",
    "# Create a ThreadPoolExecutor with maximum concurrency of 5\n",
    "with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "    # Submit tasks to the executor\n",
    "    executor.map(fetch_data, test)\n",
    "\n",
    "# Save the responses to a JSON file\n",
    "with open('categorical.json', 'w') as file:\n",
    "    json.dump(responses, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
